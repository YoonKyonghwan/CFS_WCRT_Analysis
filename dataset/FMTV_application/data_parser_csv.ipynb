{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate classes in datastructure package from xsd files(schema)\n",
    "\n",
    "import os\n",
    "\n",
    "package_name = \"datastructure\"\n",
    "xsd_path = \"taskSetInfo/xsd/\"\n",
    "if not os.path.exists(package_name):\n",
    "    !xsdata {xsd_path} --recursive --package {package_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize parser and set variables\n",
    "\n",
    "from pathlib import Path\n",
    "from datastructure import *\n",
    "from xsdata.formats.dataclass.parsers import XmlParser\n",
    "\n",
    "# load the xml file\n",
    "parser = XmlParser()\n",
    "\n",
    "org_data_dir = \"FMTV_model\"\n",
    "generated_data_dir = \"generated_FMTV_data\"\n",
    "\n",
    "if not os.path.exists(generated_data_dir):\n",
    "    os.makedirs(generated_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parse core-processor mapping\n",
    "\n",
    "import csv\n",
    "\n",
    "mapping_xml = org_data_dir + \"/mappingModel.xml\"\n",
    "mapping_model = parser.from_path(Path(mapping_xml), MappingModel)\n",
    "\n",
    "mapping_info_file_path = generated_data_dir + \"/task_core_mapping.csv\"\n",
    "if os.path.exists(mapping_info_file_path):\n",
    "    os.remove(mapping_info_file_path)\n",
    "    \n",
    "# write task_name and core_index to csv file\n",
    "for item in mapping_model.process_allocation:\n",
    "    task_name = item.process.split(\"?\")[0]\n",
    "    core_index = item.scheduler.split(\"?\")[0][-1]\n",
    "    with open(mapping_info_file_path, mode='a') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([task_name, core_index])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define functions to parse execution time of tasks\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def findRunnableByName(runnables, runnable_name):\n",
    "    return next((x for x in runnables if x.name == runnable_name), None)\n",
    "\n",
    "def findLabelByName(labels, label_name):\n",
    "    return next((x for x in labels if x.name == label_name), None)\n",
    "\n",
    "def findStimuliModelByName(stimuls, stimul_name):\n",
    "    return next((x for x in stimuls if x.name == stimul_name), None)\n",
    "\n",
    "def getCycleForMemAccess(labels, label_name):\n",
    "    number_bits = findLabelByName(labels, label_name).size.number_bits\n",
    "    if number_bits % 32 == 0: # read 32 bits at a time\n",
    "        number_access = int(number_bits / 32)\n",
    "    else:\n",
    "        number_access = int(number_bits / 32) + 1\n",
    "    return number_access * 9 # 9 cycles to read/write 32 bits\n",
    "\n",
    "def gamma_parameters(min_value, max_value, p_remain_promille, p1):    \n",
    "    p2 = 1 - p_remain_promille\n",
    "    if p1 > p2:\n",
    "        (p1, p2) = (p2, p1)\n",
    "        (min_value, max_value) = (max_value, min_value)\n",
    "        \n",
    "    alpha=(np.log(-np.log(p_remain_promille))-np.log(-np.log(1-p1)))/(np.log(max_value)-np.log(min_value))\n",
    "    beta=min_value/(-np.log(1-p1))**(1/alpha)\n",
    "    \n",
    "    return (alpha, beta)\n",
    "\n",
    "def show_hist_sample(sample):\n",
    "    plt.hist(sample, bins=100, density=True)\n",
    "    plt.show()\n",
    "\n",
    "def samplingFromWeibullDistribution(num_samples, min_value, max_value, mean_value, p_remain_promille):\n",
    "    prob_under_lower_bound = 0.1 # initial value\n",
    "    average_state = 0 # 0: init, 1: mean > sample_mean, 2: mean < sample_mean\n",
    "    while True:\n",
    "        shape_p, scale_p = gamma_parameters(min_value, max_value, p_remain_promille, prob_under_lower_bound)\n",
    "        sample = np.random.weibull(shape_p, num_samples) * scale_p\n",
    "        #round to nearest integer and convert to int\n",
    "        \n",
    "        if min_value - np.min(sample) > 0:\n",
    "            adj = min_value - np.min(sample)\n",
    "            sample += adj\n",
    "        sample = sample.clip(min_value, max_value)\n",
    "        sample = np.rint(sample).astype(int)\n",
    "        \n",
    "        # break conditions\n",
    "        if np.mean(sample) > mean_value:\n",
    "            prob_under_lower_bound += 0.05 # if prob_under_lower_bound go high, sample_mean go low\n",
    "            if average_state == 1: # prior_state = (mean > sample_mean)\n",
    "                break\n",
    "            else:\n",
    "                average_state = 2 \n",
    "        else:\n",
    "            prob_under_lower_bound /= 2 # if prob_under_lower_bound go high, sample_mean go low\n",
    "            if average_state == 2: # prior_state = (mean < sample_mean)\n",
    "                break\n",
    "            else:\n",
    "                average_state = 1 \n",
    "        \n",
    "        if prob_under_lower_bound > 0.9 or prob_under_lower_bound < 0.0001:\n",
    "            break\n",
    "        \n",
    "    # show_hist_sample(sample)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parse execution time of tasks\n",
    "\n",
    "import shutil\n",
    "\n",
    "sw_xml = org_data_dir + \"/swModel.xml\"\n",
    "sw_model = parser.from_path(Path(sw_xml), SwModel)\n",
    "\n",
    "stimulated_xml = org_data_dir + \"/stimuliModel.xml\"\n",
    "stimulated_model = parser.from_path(Path(stimulated_xml), StimuliModel)\n",
    "\n",
    "tasks = sw_model.tasks\n",
    "runnables = sw_model.runnables\n",
    "labels = sw_model.labels\n",
    "stimulis = stimulated_model.stimuli\n",
    "\n",
    "task_execution_time_files_dir = generated_data_dir + \"/task_execution_time\"\n",
    "if os.path.exists(task_execution_time_files_dir):\n",
    "    shutil.rmtree(task_execution_time_files_dir)\n",
    "os.makedirs(task_execution_time_files_dir)\n",
    "\n",
    "# variables for sampling\n",
    "num_population = 10000\n",
    "simulation_period = 60000 # 60 sec\n",
    "\n",
    "for task in tasks:\n",
    "    # variable for sampling\n",
    "    stimuli_name = task.stimuli.split(\"?\")[0]\n",
    "    task_stimuli = findStimuliModelByName(stimulis, stimuli_name)\n",
    "    if task_stimuli.recurrence is not None: #periodic task\n",
    "        recurrence_value = task_stimuli.recurrence.value\n",
    "        recurrence_unit = task_stimuli.recurrence.unit\n",
    "    else:\n",
    "        stimulus_deviation = task_stimuli.stimulus_deviation\n",
    "        assert stimulus_deviation is not None\n",
    "        recurrence_value = stimulus_deviation.lower_bound.value\n",
    "        recurrence_unit = stimulus_deviation.lower_bound.unit\n",
    "        \n",
    "    if recurrence_unit == \"us\":\n",
    "        recurrence_value = recurrence_value / 1000\n",
    "    else:\n",
    "        assert recurrence_unit == \"ms\"\n",
    "    num_samples = int(simulation_period / recurrence_value)\n",
    "    \n",
    "    for call in task.call_graph.graph_entries.calls:\n",
    "        runnable_name = call.runnable.split(\"?\")[0]\n",
    "        # find runnable by name\n",
    "        runnable = findRunnableByName(runnables, runnable_name)\n",
    "        runnable_read_cycle = 0\n",
    "        runnable_write_cycle = 0\n",
    "        num_execution_item = 0\n",
    "        for runnable_item in runnable.runnable_items:\n",
    "            if runnable_item.access == \"read\":\n",
    "                label_name = runnable_item.data.split(\"?\")[0]\n",
    "                runnable_read_cycle += getCycleForMemAccess(labels, label_name)\n",
    "            elif runnable_item.access == \"write\":\n",
    "                label_name = runnable_item.data.split(\"?\")[0]\n",
    "                runnable_write_cycle += getCycleForMemAccess(labels, label_name)\n",
    "            else:\n",
    "                num_execution_item += 1\n",
    "                min_value = runnable_item.deviation.lower_bound.value\n",
    "                max_value = runnable_item.deviation.upper_bound.value\n",
    "                mean_value = runnable_item.deviation.distribution.mean.value\n",
    "                p_remain_promille = runnable_item.deviation.distribution.p_remain_promille\n",
    "                \n",
    "                if num_population > num_samples:\n",
    "                    population = samplingFromWeibullDistribution(num_population, min_value, max_value, mean_value, p_remain_promille)\n",
    "                    sample = np.random.choice(population, num_samples)\n",
    "                else:\n",
    "                    sample = samplingFromWeibullDistribution(num_samples, min_value, max_value, mean_value, p_remain_promille)\n",
    "        \n",
    "        assert num_execution_item == 1\n",
    "        \n",
    "        # convert cycle to time (system clock = 200MHz, 1 cycle = 5ns)\n",
    "        runnable_read_time = runnable_read_cycle * 5\n",
    "        runnable_write_time = runnable_read_cycle * 5\n",
    "        sample = sample * 5\n",
    "        \n",
    "        data = sample.tolist()\n",
    "        data.insert(0, runnable_write_time)\n",
    "        data.insert(0, runnable_read_time)\n",
    "        \n",
    "        task_info_file_path = task_execution_time_files_dir + \"/\" + task.name + \".csv\"\n",
    "        with open(task_info_file_path, mode='a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            # read, write, executions\n",
    "            writer.writerow(data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parse period of periodic tasks and inter-arrival time of sporadic tasks\n",
    "\n",
    "if tasks is None:\n",
    "    sw_xml = org_data_dir + \"/swModel.xml\"\n",
    "    sw_model = parser.from_path(Path(sw_xml), SwModel)\n",
    "    tasks = sw_model.tasks\n",
    "\n",
    "if stimulis is None:\n",
    "    stimulated_xml = org_data_dir + \"/stimuliModel.xml\"\n",
    "    stimulated_model = parser.from_path(Path(stimulated_xml), StimuliModel)\n",
    "    stimulis = stimulated_model.stimuli\n",
    "\n",
    "task_period_file_path = generated_data_dir + \"/task_period.csv\"\n",
    "\n",
    "for task in tasks:\n",
    "    task_name = task.name\n",
    "    stimuli_name = task.stimuli.split(\"?\")[0]\n",
    "    task_stimuli = findStimuliModelByName(stimulis, stimuli_name)\n",
    "    if task_stimuli.recurrence is not None: #periodic task\n",
    "        period = task_stimuli.recurrence.value\n",
    "        recurrence_unit = task_stimuli.recurrence.unit\n",
    "        \n",
    "        if recurrence_unit == \"us\":\n",
    "            period = period * 1000 # convert to ns\n",
    "        else:\n",
    "            assert recurrence_unit == \"ms\"\n",
    "            period = period * 1000000 # convert to ns\n",
    "        with open(task_period_file_path, mode='a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([task.name, period])\n",
    "            \n",
    "    else: #sporadic task\n",
    "        stimulus_deviation = task_stimuli.stimulus_deviation\n",
    "        assert stimulus_deviation is not None\n",
    "        lower_bound = stimulus_deviation.lower_bound.value\n",
    "        upper_bound = stimulus_deviation.upper_bound.value\n",
    "        recurrence_unit = stimulus_deviation.lower_bound.unit\n",
    "        \n",
    "        if recurrence_unit == \"us\":\n",
    "            lower_bound = lower_bound * 1000\n",
    "            upper_bound = upper_bound * 1000\n",
    "        else:\n",
    "            assert recurrence_unit == \"ms\"\n",
    "            lower_bound = lower_bound * 1000000\n",
    "            upper_bound = upper_bound * 1000000\n",
    "        with open(task_period_file_path, mode='a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([task.name, lower_bound, upper_bound])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge runnable's execution time to make the phased task model\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "org_directory = \"./generated_FMTV_data/task_execution_time/\"\n",
    "phased_directory = \"./generated_FMTV_data/task_execution_time_phased/\"\n",
    "\n",
    "if os.path.exists(phased_directory):\n",
    "    shutil.rmtree(phased_directory)\n",
    "os.makedirs(phased_directory)\n",
    "\n",
    "for filename in os.listdir(org_directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # read csv to numpy array\n",
    "        runnable_info = np.genfromtxt(org_directory + filename, delimiter=',')\n",
    "        # sum columns\n",
    "        task_info = np.sum(runnable_info, axis=0) \n",
    "        task_info = task_info.tolist()\n",
    "        # save to new file\n",
    "        with open(phased_directory + filename, mode='a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(task_info)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
