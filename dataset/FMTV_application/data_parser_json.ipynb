{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate classes in datastructure package from xsd files(schema)\n",
    "\n",
    "import os\n",
    "\n",
    "package_name = \"datastructure\"\n",
    "xsd_path = \"taskSetInfo/xsd/\"\n",
    "if not os.path.exists(package_name):\n",
    "    !xsdata {xsd_path} --recursive --package {package_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize parser and set variables\n",
    "from pathlib import Path\n",
    "from datastructure import *\n",
    "from xsdata.formats.dataclass.parsers import XmlParser\n",
    "import shutil\n",
    "\n",
    "# load the xml file\n",
    "parser = XmlParser()\n",
    "\n",
    "org_data_dir = \"FMTV_model\"\n",
    "generated_data_dir = \"generated_FMTV_json\"\n",
    "\n",
    "if os.path.exists(generated_data_dir):\n",
    "    shutil.rmtree(generated_data_dir)\n",
    "os.makedirs(generated_data_dir)\n",
    "\n",
    "tasks_info = []\n",
    "\n",
    "def find_task_info(tasks_info, name):\n",
    "    for task_info in tasks_info:\n",
    "        if task_info['task_name'] == name:\n",
    "            return task_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parse core-processor mapping\n",
    "\n",
    "import csv\n",
    "\n",
    "mapping_xml = org_data_dir + \"/mappingModel.xml\"\n",
    "mapping_model = parser.from_path(Path(mapping_xml), MappingModel)\n",
    "    \n",
    "# write task_name and core_index to csv file\n",
    "for item in mapping_model.process_allocation:\n",
    "    task_name = item.process.split(\"?\")[0]\n",
    "    core_index = int(item.scheduler.split(\"?\")[0][-1])\n",
    "    tasks_info.append({\"task_name\":task_name, \"core_index\":core_index})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define functions to parse execution time of tasks\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def findRunnableByName(runnables, runnable_name):\n",
    "    return next((x for x in runnables if x.name == runnable_name), None)\n",
    "\n",
    "def findLabelByName(labels, label_name):\n",
    "    return next((x for x in labels if x.name == label_name), None)\n",
    "\n",
    "def findStimuliModelByName(stimuls, stimul_name):\n",
    "    return next((x for x in stimuls if x.name == stimul_name), None)\n",
    "\n",
    "def getCycleForMemAccess(labels, label_name):\n",
    "    number_bits = findLabelByName(labels, label_name).size.number_bits\n",
    "    if number_bits % 32 == 0: # read 32 bits at a time\n",
    "        number_access = int(number_bits / 32)\n",
    "    else:\n",
    "        number_access = int(number_bits / 32) + 1\n",
    "    return number_access * 9 # 9 cycles to read/write 32 bits\n",
    "\n",
    "def gamma_parameters(min_value, max_value, p_remain_promille, p1):    \n",
    "    p2 = 1 - p_remain_promille\n",
    "    if p1 > p2:\n",
    "        (p1, p2) = (p2, p1)\n",
    "        (min_value, max_value) = (max_value, min_value)\n",
    "        \n",
    "    alpha=(np.log(-np.log(p_remain_promille))-np.log(-np.log(1-p1)))/(np.log(max_value)-np.log(min_value))\n",
    "    beta=min_value/(-np.log(1-p1))**(1/alpha)\n",
    "    \n",
    "    return (alpha, beta)\n",
    "\n",
    "def show_hist_sample(sample):\n",
    "    plt.hist(sample, bins=100, density=True)\n",
    "    plt.show()\n",
    "\n",
    "def samplingFromWeibullDistribution(num_samples, min_value, max_value, mean_value, p_remain_promille):\n",
    "    prob_under_lower_bound = 0.1 # initial value\n",
    "    average_state = 0 # 0: init, 1: mean > sample_mean, 2: mean < sample_mean\n",
    "    while True:\n",
    "        shape_p, scale_p = gamma_parameters(min_value, max_value, p_remain_promille, prob_under_lower_bound)\n",
    "        sample = np.random.weibull(shape_p, num_samples) * scale_p\n",
    "        #round to nearest integer and convert to int\n",
    "        \n",
    "        if min_value - np.min(sample) > 0:\n",
    "            adj = min_value - np.min(sample)\n",
    "            sample += adj\n",
    "        sample = sample.clip(min_value, max_value)\n",
    "        sample = np.rint(sample).astype(int)\n",
    "        \n",
    "        # break conditions\n",
    "        if np.mean(sample) > mean_value:\n",
    "            prob_under_lower_bound += 0.05 # if prob_under_lower_bound go high, sample_mean go low\n",
    "            if average_state == 1: # prior_state = (mean > sample_mean)\n",
    "                break\n",
    "            else:\n",
    "                average_state = 2 \n",
    "        else:\n",
    "            prob_under_lower_bound /= 2 # if prob_under_lower_bound go high, sample_mean go low\n",
    "            if average_state == 2: # prior_state = (mean < sample_mean)\n",
    "                break\n",
    "            else:\n",
    "                average_state = 1 \n",
    "        \n",
    "        if prob_under_lower_bound > 0.9 or prob_under_lower_bound < 0.0001:\n",
    "            break\n",
    "        \n",
    "    # show_hist_sample(sample)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = samplingFromWeibullDistribution(100000, 12375, 213419, 82342, 0.005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parse execution time of tasks\n",
    "\n",
    "sw_xml = org_data_dir + \"/swModel.xml\"\n",
    "sw_model = parser.from_path(Path(sw_xml), SwModel)\n",
    "\n",
    "stimulated_xml = org_data_dir + \"/stimuliModel.xml\"\n",
    "stimulated_model = parser.from_path(Path(stimulated_xml), StimuliModel)\n",
    "\n",
    "tasks = sw_model.tasks\n",
    "runnables = sw_model.runnables\n",
    "labels = sw_model.labels\n",
    "stimulis = stimulated_model.stimuli\n",
    "\n",
    "# variables for sampling\n",
    "num_population = 10000\n",
    "simulation_period_ms = 60000 # 60 sec\n",
    "\n",
    "for task in tasks:\n",
    "    # variable for sampling\n",
    "    stimuli_name = task.stimuli.split(\"?\")[0]\n",
    "    task_stimuli = findStimuliModelByName(stimulis, stimuli_name)\n",
    "    if task_stimuli.recurrence is not None: #periodic task\n",
    "        recurrence_value = task_stimuli.recurrence.value\n",
    "        recurrence_unit = task_stimuli.recurrence.unit\n",
    "    else:\n",
    "        stimulus_deviation = task_stimuli.stimulus_deviation\n",
    "        assert stimulus_deviation is not None\n",
    "        recurrence_value = stimulus_deviation.lower_bound.value\n",
    "        recurrence_unit = stimulus_deviation.lower_bound.unit\n",
    "        \n",
    "    if recurrence_unit == \"us\":\n",
    "        recurrence_value = recurrence_value / 1000\n",
    "    else:\n",
    "        assert recurrence_unit == \"ms\"\n",
    "    num_samples = int(simulation_period_ms / recurrence_value)\n",
    "    \n",
    "    runnable_info = []\n",
    "    for call in task.call_graph.graph_entries.calls:\n",
    "        runnable_name = call.runnable.split(\"?\")[0]\n",
    "        # find runnable by name\n",
    "        runnable = findRunnableByName(runnables, runnable_name)\n",
    "        runnable_read_cycle = 0\n",
    "        runnable_write_cycle = 0\n",
    "        num_execution_item = 0\n",
    "        for runnable_item in runnable.runnable_items:\n",
    "            if runnable_item.access == \"read\":\n",
    "                label_name = runnable_item.data.split(\"?\")[0]\n",
    "                runnable_read_cycle += getCycleForMemAccess(labels, label_name)\n",
    "            elif runnable_item.access == \"write\":\n",
    "                label_name = runnable_item.data.split(\"?\")[0]\n",
    "                runnable_write_cycle += getCycleForMemAccess(labels, label_name)\n",
    "            else:\n",
    "                num_execution_item += 1\n",
    "                min_value = runnable_item.deviation.lower_bound.value\n",
    "                max_value = runnable_item.deviation.upper_bound.value\n",
    "                mean_value = runnable_item.deviation.distribution.mean.value\n",
    "                p_remain_promille = runnable_item.deviation.distribution.p_remain_promille\n",
    "                \n",
    "                if num_population > num_samples:\n",
    "                    population = samplingFromWeibullDistribution(num_population, min_value, max_value, mean_value, p_remain_promille)\n",
    "                    sample = np.random.choice(population, num_samples)\n",
    "                else:\n",
    "                    sample = samplingFromWeibullDistribution(num_samples, min_value, max_value, mean_value, p_remain_promille)\n",
    "        \n",
    "        assert num_execution_item == 1\n",
    "        \n",
    "        # convert cycle to time (system clock = 200MHz, 1 cycle = 5ns[])\n",
    "        runnable_read_time = runnable_read_cycle * 9\n",
    "        runnable_write_time = runnable_write_cycle * 9\n",
    "        runnable_execution_time = (sample).tolist()\n",
    "        \n",
    "        runnable_info.append({\"read\":runnable_read_time, \"write\":runnable_write_time, \"execution\":runnable_execution_time})\n",
    "        \n",
    "    task_info = find_task_info(tasks_info, task.name)\n",
    "    task_info['priority'] = task.priority\n",
    "    task_info['time_ns'] = runnable_info\n",
    "    task_info['isRTTask'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parse period of periodic tasks and inter-arrival time of sporadic tasks\n",
    "\n",
    "if tasks is None:\n",
    "    sw_xml = org_data_dir + \"/swModel.xml\"\n",
    "    sw_model = parser.from_path(Path(sw_xml), SwModel)\n",
    "    tasks = sw_model.tasks\n",
    "\n",
    "if stimulis is None:\n",
    "    stimulated_xml = org_data_dir + \"/stimuliModel.xml\"\n",
    "    stimulated_model = parser.from_path(Path(stimulated_xml), StimuliModel)\n",
    "    stimulis = stimulated_model.stimuli\n",
    "\n",
    "task_period_file_path = generated_data_dir + \"/task_period.csv\"\n",
    "\n",
    "for task in tasks:\n",
    "    task_name = task.name\n",
    "    stimuli_name = task.stimuli.split(\"?\")[0]\n",
    "    task_stimuli = findStimuliModelByName(stimulis, stimuli_name)\n",
    "    if task_stimuli.recurrence is not None: #periodic task\n",
    "        period = task_stimuli.recurrence.value\n",
    "        recurrence_unit = task_stimuli.recurrence.unit\n",
    "        \n",
    "        if recurrence_unit == \"us\":\n",
    "            period = period * 1000 # convert to ns\n",
    "        else:\n",
    "            assert recurrence_unit == \"ms\"\n",
    "            period = period * 1000000 # convert to ns\n",
    "        \n",
    "        task_info = find_task_info(tasks_info, task_name)\n",
    "        task_info['isPeriodic'] = True\n",
    "        task_info['period_ns'] = period\n",
    "            \n",
    "    else: #sporadic task\n",
    "        stimulus_deviation = task_stimuli.stimulus_deviation\n",
    "        assert stimulus_deviation is not None\n",
    "        lower_bound = stimulus_deviation.lower_bound.value\n",
    "        upper_bound = stimulus_deviation.upper_bound.value\n",
    "        recurrence_unit = stimulus_deviation.lower_bound.unit\n",
    "        \n",
    "        if recurrence_unit == \"us\":\n",
    "            lower_bound = lower_bound * 1000\n",
    "            upper_bound = upper_bound * 1000\n",
    "        else:\n",
    "            assert recurrence_unit == \"ms\"\n",
    "            lower_bound = lower_bound * 1000000\n",
    "            upper_bound = upper_bound * 1000000\n",
    "        \n",
    "        task_info = find_task_info(tasks_info, task_name)\n",
    "        task_info['isPeriodic'] = False # sporadic task\n",
    "        task_info['lower_bound_ns'] = lower_bound\n",
    "        task_info['upper_bound_ns'] = upper_bound\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge runnable's execution time to make the phased task model\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "for task_info in tasks_info:\n",
    "    phased_read_time = 0\n",
    "    phased_write_time = 0\n",
    "    num_samples = len(tasks_info[0]['time_ns'][0]['execution'])\n",
    "    phased_execution_time = [0]* num_samples\n",
    "    for runnable_info in task_info['time_ns']:\n",
    "        phased_read_time += runnable_info['read']\n",
    "        phased_write_time += runnable_info['write']\n",
    "        phased_execution_time = [x + y for x, y in zip(phased_execution_time, runnable_info['execution'])]\n",
    "        \n",
    "    task_info['phased_read'] = phased_read_time\n",
    "    task_info['phased_write'] = phased_write_time\n",
    "    task_info['phased_execution'] = phased_execution_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert Non_RT_Task\n",
    "for i in range(4):\n",
    "    Non_RT_task_info={}\n",
    "    Non_RT_task_info['core_index'] = i\n",
    "    Non_RT_task_info['task_name'] = \"Non_RT_Task_1000ms_core\" + str(Non_RT_task_info['core_index'])\n",
    "    Non_RT_task_info['isRTTask'] = False\n",
    "    Non_RT_task_info['isPeriodic'] = True\n",
    "    Non_RT_task_info['period_ns'] = 1000000000\n",
    "    Non_RT_task_info['phased_read'] = 0\n",
    "    Non_RT_task_info['phased_write'] = 0\n",
    "    num_samples = (int)(simulation_period_ms / Non_RT_task_info['period_ns'] * 1000000)\n",
    "    Non_RT_task_info['phased_execution'] = [100000000]* num_samples # execution time is 100ms\n",
    "    Non_RT_task_info['time_ns'] = []\n",
    "    Non_RT_task_info['time_ns'].append(\n",
    "                                        {'read':Non_RT_task_info['phased_read'], \n",
    "                                        'write':Non_RT_task_info['phased_write'], \n",
    "                                        'execution':Non_RT_task_info['phased_execution']}\n",
    "                                        )\n",
    "    tasks_info.append(Non_RT_task_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if task name is Task_10ms or Task_20ms, remove it\n",
    "# for task_info in tasks_info:\n",
    "#     if task_info['task_name'] == \"Task_10ms\" or task_info['task_name'] == \"Task_20ms\":\n",
    "#         tasks_info.remove(task_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(generated_data_dir + \"/tasks_info.json\", \"w\") as f:\n",
    "    json.dump(tasks_info, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tasks_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'core_index': 1,\n",
       " 'task_name': 'Non_RT_Task_1000ms_core1',\n",
       " 'isRTTask': False,\n",
       " 'isPeriodic': True,\n",
       " 'period_ns': 1000000000,\n",
       " 'phased_read': 0,\n",
       " 'phased_write': 0,\n",
       " 'phased_execution': [100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000,\n",
       "  100000000],\n",
       " 'time_ns': [{'read': 0,\n",
       "   'write': 0,\n",
       "   'execution': [100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000,\n",
       "    100000000]}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = task_info = find_task_info(tasks_info, \"Non_RT_Task_1000ms_core1\")\n",
    "temp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
